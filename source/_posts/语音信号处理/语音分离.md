---
title: 语音分离
date: 2024/09/27 23:06:22
tags:
  - 语音分离
categories:
  - 语音信号处理
share: "true"
---


# 方法

## 传统

### 基于统计

高斯混合模型、隐马尔可夫模型

假定干扰语音信号与目标语音信号在统计上无关， 采用诸如复高斯或独立成分分析等概率分布模型对目标语音信号进行建模

### 基于聚类

计算听觉分析（Computational Auditory Sense Analysis, CASA）和频谱聚类

基于听觉分析的原理，通过对目标语音信号的基音和连续性等特性进行估计，从而将目标
语音与干扰语音区分开来，实现类似于人类听觉系统的分离


### 基于分解

利用非负矩阵分解（Nonnegative Matrix Factorization, NMF）的原理，将语音分离问题描述成矩阵分解问题，将混合语音信号的时频（Time-Frequency, T-F）表示分解成基本信号与激励信号的结合，然后将每个基本信号学习到的激励信号用于重构目标语音信号。


## 深度学习

### DNN

第一个使用深度学习技术解决 说话人分离问题的方法

DNN 可以通过连接语音信号的相邻特征提取出有用的信息，比如上下文信息、幅 度谱、梅尔频率倒谱系数（Mel-Frequency Cepstral Coefficient, MFCC）以及其他特征参数。

### RNN

由于增加连接窗口大小的限制，连接相邻特征数量的增加，会增加神经网络模型的复杂性。因此，有研究人员使用循环神经网络（Recurrent Neural Network, RNN）代替 DNN，以提取语音信号的时间信息。


### DPRNN

双路径循环神经网络（Dual-Path Recurrent Neural Network, DPRNN）

有效地处理超长时间序列，对较长的连续输入进行分解， 并对其进行块内（局部）运算和块间（全局）运算，使得在每一个运算中的输入量都与原来 的序列长度的平方根成比例，实现次线性处理，缓解了优化的难题。


### DPTNet

双路径 Transformer 网络（Dual-Path Transformer Network）

引入了一种改进的 Transformer，在原始的 Transformer 模型中添 加了 RNN，可以在不使用位置编码的情况下，提取出语音序列中的次序信息，使得网络可以用更直观的上下文信息对语音序列进行刻画，因此可以得到较好的分离效果。



### SepFormer

Transformer 的无 RNN 的网络模型。

与性能相当的语音分离系统相比，它的速度要快得多，对电脑内存的要求也更低。


### Conv-TasNet

全卷时域语音分离网络（ Convolution Time-Domain Audio Separation Net）


### GAN







# source


### 2020

如何分离不同说话人的语音信号？深度学习单通道语音分离方法最新综述
https://zhuanlan.zhihu.com/p/194329601

从鸡尾酒会问题入门语音分离
https://zhuanlan.zhihu.com/p/142143151

### 2022

语音分离综述
https://zhuanlan.zhihu.com/p/453975913


### 2023

TDANet: 一种具有自上而下注意力的用于语音分离的高效自编码器架构 （ICLR 2023）
https://zhuanlan.zhihu.com/p/605100121




# paper

https://github.com/JusperLee/Speech-Separation-Paper-Tutorial


语音分离的文章分享
https://zhuanlan.zhihu.com/p/359514812

# model

SpeechBrain了解一下？12月或者1月beta-release哦

频域：

时域：TasNet、DPRNN、TDANet、Sepformer

https://github.com/JusperLee?tab=repositories



# conf

icassp、interspeech

TASLP

**NeurIPS/ICLR/TPAMI/ICASSP/Interspeech**

>题主首先要明确自己先从声源分离入手，还是语音识别入手，还是多说话人语音识别入手。这三者在github上都有比较好的开源，且相对较新，很能跟上学术界的步伐。会议可以关注ICASSP和INTERSPEECH，期刊可以关注The_IEEE_/ACM Transactions _on Audio_, Speech, and Language Processing，speech communication，signal processing letter。



>因为不像其他机器学习的领域，大家积极开放数据库和代码，统一衡量标准，语音界很多团队还在自娱自乐，同一个团队同样的数据发表在上述顶会上的文章的metric的数据量级都会不一致，更不用说在github上要找个好的语音模型的代码有多难（除了那几家大厂还良心发布代码和数据）。 CV和NLP的顶会文章下限至少比语音的高不少，上限大家都很高。而且，50%的命中率真的是代表难度不大，大家可以看看台大李宏毅的本科生上完他的课把课堂作业拿去投这两会能中的也挺多。至于顶刊TASLP，评委的友好程度高过其他方向太多，当然，也导致很多不怎么靠谱的文章最终被接收了。

>语音领域没有cv,nlp做的那么大，语音顶会只有icassp和interspeech，做的人少，而且没有其他领域有一套属于自己的网络或者架构，现在所用的结构基本都是从nlp和cv搬过来的


# mass

关于语音分离/语音前端处理社群建立
https://zhuanlan.zhihu.com/p/134797981

